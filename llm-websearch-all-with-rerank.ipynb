{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Web Search\n",
    "\n",
    "> *This notebook should work well with the **`conda_python3`** kernel in SageMaker Studio*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we show you how to:\n",
    "- Define a tool that the LLM can reliably call that produces JSON output\n",
    "- Use the googlesearch and wikipedia python modules to search the internet if the LLM cannot answer a research question itself\n",
    "- Rerank the search results options from best to worst\n",
    "- Scrape and process the best option HTML page to create context for the LLM\n",
    "- Create a Bedrock Guardrail \n",
    "- Use the Guardrail in your calls to the Bedrock API\n",
    "\n",
    "We will use Bedrock's `Claude 3.5 Sonnet` and `Claude 3 Haiku` base models using the AWS boto3 SDK. \n",
    "\n",
    "> **Note:** *This notebook can be used in SageMaker Studio or run locally if you setup your AWS credentials.*\n",
    "\n",
    "#### Prerequisites\n",
    "- This notebook requires permissions to access Amazon Bedrock\n",
    "- Ensure you have gone to the Bedrock models access page and enabled acceess to `Anthropic Claude 3.5 Sonnet` and `Claude 3 Haiku`\n",
    "- AmazonBedrockFullAccess\n",
    "> **Note:** If you are running this notebook without an Admin role, make sure that your notebook's role includes the following managed policies:\n",
    "\n",
    "#### Use case\n",
    "You are building a research assistant GenAI application. In some cases the user's question may be about an event, product, or service that is more recent than the cutoff training date for the LLM model or not within the model's knowledge. For these cases, we want the LLM model to call the internet search tool to gather context relating to the question. Then we can supply that context back to the LLM to answer the question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook setup\n",
    "\n",
    "1. If you are attending an instructor lead workshop or deployed the workshop infrastructure using the provided [CloudFormation Template](https://raw.githubusercontent.com/aws-samples/xxx/main/cloudformation/workshop-v1-final-cfn.yml) you can proceed to step 2, otherwise you will need to download the workshop [GitHub Repository](https://github.com/aws-samples/xxx) to your local machine.\n",
    "\n",
    "2. Install the required dependencies by running the pip install commands in the next cell.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "> ‚ö†Ô∏è **Please ignore error messages related to pip's dependency resolver.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip** You can use `Shift + Enter` to execute the cell and move to the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU pip\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import string\n",
    "import pprint\n",
    "import random\n",
    "from googlesearch import search\n",
    "import wikipedia\n",
    "from bs4 import BeautifulSoup\n",
    "from botocore.exceptions import ClientError\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "session = boto3.Session()\n",
    "region = session.region_name\n",
    "\n",
    "# Change which line is uncommented below to select the LLM model you want to use\n",
    "#modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "#modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "modelId = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "\n",
    "print(f\"Using modelId: {modelId}\")\n",
    "print(f\"Using region: {region}\")\n",
    "print('Running boto3 version:', boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `modelId` and `region` variables defined in the above cell will be used throughout the workshop. Just make sure to run the cells from top to bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Boto3 SDK & the Converse API\n",
    "We will be using the [Amazon Boto3 SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime.html) and the [Converse API](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html) throughout this workshop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a boto3 runtime client for calling the LLM and create a boto3 admin client for creating our Guardrail\n",
    "bedrock_runtime_client = boto3.client(service_name = 'bedrock-runtime', region_name = region,)\n",
    "bedrock_admin_client = boto3.client('bedrock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking the LLM questions without an internet search tool\n",
    "\n",
    "Let's start out by asking some questions to the LLM without supplying the option of an internet search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function answers the user's questions directly from the LLM's knowledge\n",
    "def answer_question(question):\n",
    "    query = f\"\"\"\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    Answer the user's question in complete sentances.\n",
    "    Skip the preamble.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": query}]}],\n",
    "        \"system\": [{ \"text\": \"You are an expert research assistant.\"}],\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": 4096,\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = bedrock_runtime_client.converse(**converse_api_params)\n",
    "    \n",
    "    # Provide the LLM's response\n",
    "    print(f\"\\nFinal answer is: {response['output']['message']['content'][-1]['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The LLM should be able to answer this question from it's own knowledge\n",
    "answer_question(\"Which country won the most gold medals in the 2020 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now try a question where the information is too new and past the LLM's training cutoff date\n",
    "answer_question(\"Which country won the most gold medals in the 2024 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is the current time and date in Seattle, WA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Searching and scraping with Wikipedia and Google\n",
    "\n",
    "In this example we create three functions:\n",
    "* handle_search\n",
    "    * This function first calls the internet provider to search for pages (Wikipedia) / URLs (Google) related to the user's question\n",
    "    * Use the `num_results` parameter to control how many pages/URLs you want returned\n",
    "    * Then it passes the list of pages to the reranker function to use the LLM to order the list pages/URLs from best choice to worst choice\n",
    "    * Finally it iterates through the list of pages/URLs to make sure content is there and passes the first (best) choice text block to the LLM as context in the prompt\n",
    "* get_wikipedia_page_content\n",
    "    * This function uses the wikipedia module to get the html content of a single Wikipedia page\n",
    "    * The markdownify module is used to transform the page markdown (including tables) to lines of text\n",
    "    * Then the text is processed to remove the standard info sections at the bottom of Wikipedia pages from the content\n",
    "* reranker\n",
    "    * This function takes the list of pages that the internet search provider returns and uses the LLM to rank them in order from best choice to worst choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the search provider variable\n",
    "search_provider = \"Wikipedia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_search(query, search_provider):\n",
    "    num_results = 5\n",
    "    # Proceed with Wikipedia search\n",
    "    print(f\"Searching {search_provider}...\\n\")\n",
    "    try:\n",
    "        if search_provider == \"Wikipedia\":\n",
    "            # Use the wikipedia module to get wiki pages related to the user's question\n",
    "            search_results = wikipedia.search(query, results=num_results)\n",
    "        elif search_provider == \"Google\":\n",
    "            # Sometimes Google will only return one page even if asked for more, try again if only one\n",
    "            search_results = ['dummy']\n",
    "            while len(search_results) == 1:\n",
    "                # Use the googlesearch module to get pages related to the user's question\n",
    "                for page in search(query, sleep_interval=5, num_results=num_results):\n",
    "                    search_results.append(page)\n",
    "                if len(search_results) != 1:\n",
    "                    break\n",
    "\n",
    "        best_options = reranker(query, search_results)\n",
    "            \n",
    "        for option in best_options:\n",
    "            print(f\"\\nScraping page: {option}\")\n",
    "            if search_provider == \"Wikipedia\":\n",
    "                content = get_wikipedia_page_content(option)\n",
    "            elif search_provider == \"Google\":\n",
    "                content = get_google_page_content(option)\n",
    "\n",
    "            if content and content != \"skip page\":\n",
    "                return content\n",
    "            else:\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error during handle search: {e}\")\n",
    "        return \"\"\n",
    "    return \"No content found on any of the pages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reranker(question, internet_search_results):\n",
    "    query = f\"\"\"\n",
    "    Given this user's question:\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    Rank from best to worst the choices that are provided in the choices tags for searching the internet to provide an answer to the user's question.\n",
    "    <choices>\n",
    "    {internet_search_results}\n",
    "    </choices>\n",
    "    Skip the preamble and do not include any reasoning in your output. \n",
    "    Simply return the choices in a JSON list from best to worst choice.\n",
    "    \"\"\"\n",
    "\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": query}]}],\n",
    "        \"system\": [{ \"text\": \"You are an expert research assistant.\"}],\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": 4096,\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = bedrock_runtime_client.converse(**converse_api_params)\n",
    "    best_options = response['output']['message']['content'][-1]['text']\n",
    "    \n",
    "    print(\"Metrics the for reranker call to the LLM:\")\n",
    "    token_usage = response['usage']\n",
    "    input_tokens = token_usage['inputTokens']\n",
    "    print(f\"Input tokens:  {input_tokens}\")\n",
    "    output_tokens = token_usage['outputTokens']\n",
    "    print(f\"Output tokens:  {output_tokens}\")\n",
    "    total_tokens = token_usage['totalTokens']\n",
    "    print(f\"Total tokens:  {total_tokens}\")\n",
    "    latency = response['metrics']['latencyMs']\n",
    "    print(f\"Latency: {latency} ms\\n\")\n",
    "\n",
    "    # Provide the LLM's response\n",
    "    print(f\"\\nThe reranked order of:\\n{internet_search_results} is:\\n{best_options}\\n\")\n",
    "    return json.loads(best_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_wikipedia_page_content(page):\n",
    "    try:\n",
    "        html_text = wikipedia.page(title=page, auto_suggest=False).html()\n",
    "        markdown_text = md(html_text)\n",
    "        payload = \"{} \\n\".format(markdown_text)\n",
    "        cleaned_markdown_text = \"\"\n",
    "        if markdown_text:\n",
    "            lines = payload.splitlines()\n",
    "            # Do not include the standard info sections at the bottom of Wikipedia pages in the content\n",
    "            # Do not include the edit links or links to images to reduce token count\n",
    "            for line in lines:\n",
    "                if line == \"\" or \"[edit]\" in line or \"[![]\" in line:\n",
    "                    continue\n",
    "                elif line == \"See also\" or line == \"References\" or line == \"External link\" or line == \"Further reading\":\n",
    "                    break\n",
    "                else:\n",
    "                    cleaned_markdown_text += line + '\\n'\n",
    "        else:\n",
    "            Print(f\"No markdown text found on page: {page}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while requesting content from {page} skipping...: {e}\")\n",
    "        return \"skip page\"\n",
    "    #print(f\"cleaned_markdown_text is:\\n{cleaned_markdown_text}\\n\")\n",
    "    \n",
    "    return cleaned_markdown_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use the Bedrock Converse API for inference and configure 'Tool Use'\n",
    "\n",
    "* Configure the tool definition\n",
    "    * This JSON schema defines our internet search tool and how the LLM should output the JSON when calling the tool\n",
    "* answer_question\n",
    "    * This function calls the LLM to answer the user's question directly or outputs 'tool use' JSON if an internet search is required\n",
    "    * Note that the LLM will have a propencity to use the tool, so we must direct it in the prompt to only do so as a last resort\n",
    "* answer_question_with_content\n",
    "    * This function answers the user's question based on the block of text supplied by the internet provider search\n",
    "    * The markdownify module is used to transform the page markdown (including tables) to lines of text\n",
    "    * Then the text is processed to remove the standard info sections at the bottom of Wikipedia pages from the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Which country won the most gold medals in the 2020 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tool definition\n",
    "provider_websearch_schema = {\n",
    "      \"toolSpec\": {\n",
    "        \"name\": \"internet_search\",\n",
    "        \"description\": \"A tool to retrieve up to date information from an internet search.\",\n",
    "        \"inputSchema\": {\n",
    "          \"json\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The users question as-is for the internet search\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"question\"]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "# In this example, we save only one tool schema to the configuration, but you could have many tools\n",
    "toolConfig = {\n",
    "    \"tools\": [provider_websearch_schema]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    query = f\"\"\"\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    You have access to the internet_search tool.\n",
    "    Take your time to first think step by step if the question could be answered within your own knowledge.\n",
    "    Output your thinking in <thinking></thinking> tags\n",
    "    Only use the internet_search tool as a last resort if you cannot answer the user's question within the question tags from your own knowledge.\n",
    "    For example, only use the internet_search_tool if the subject, product, or event is more recent than your training cutoff date.\n",
    "    \n",
    "    Skip the preamble.\n",
    "    \"\"\"\n",
    "\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": query}]}],\n",
    "        \"toolConfig\": toolConfig,\n",
    "        \"system\": [{ \"text\": \"You are an expert research assistant.\"}],\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": 4096,\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = bedrock_runtime_client.converse(**converse_api_params)\n",
    "    print(\"Metrics the for first call to the LLM:\")\n",
    "    token_usage = response['usage']\n",
    "    input_tokens = token_usage['inputTokens']\n",
    "    print(f\"Input tokens:  {input_tokens}\")\n",
    "    output_tokens = token_usage['outputTokens']\n",
    "    print(f\"Output tokens:  {output_tokens}\")\n",
    "    total_tokens = token_usage['totalTokens']\n",
    "    print(f\"Total tokens:  {total_tokens}\")\n",
    "    latency = response['metrics']['latencyMs']\n",
    "    print(f\"Latency: {latency} ms\\n\")\n",
    "    \n",
    "    # Check the LLM's response to see if it answered the question or needs to use the internet search tool\n",
    "    internet_search = None\n",
    "    for content in response['output']['message']['content']:\n",
    "        if isinstance(content, dict) and 'toolUse' in content:\n",
    "            tool_use = content['toolUse']\n",
    "            if tool_use['name'] == \"internet_search\":\n",
    "                internet_search = tool_use['input']\n",
    "                break\n",
    "\n",
    "    if internet_search:\n",
    "        question = internet_search[\"question\"]\n",
    "        # Call the function to get the content from the internet\n",
    "        content = handle_search(question, search_provider)\n",
    "        if content:\n",
    "            print(\"\\nInternet search successful\")\n",
    "            response = answer_question_with_content(question, content)\n",
    "            print(f\"\\nFinal answer = {response['output']['message']['content'][-1]['text']}\\n\")\n",
    "        else:\n",
    "            print(\"No content found from internet search\")\n",
    "    else:\n",
    "        print(\"No internet search needed.\")\n",
    "        answer = response['output']['message']['content'][-1]['text']\n",
    "        answer = answer.split(\"</thinking>\")[-1]\n",
    "        print(f\"Final answer is: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def answer_question_with_content(question, content):\n",
    "    query = f\"\"\"\n",
    "    Based solely on this content:\n",
    "    <content>\n",
    "    {content}\n",
    "    </content>\n",
    "    Answer this question:\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "    Skip any preamble or references to the tool.\n",
    "    \"\"\"\n",
    "\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": query}]}],\n",
    "        \"system\": [{ \"text\": \"You are an expert research assistant.\" }],\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": 4096,\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = bedrock_runtime_client.converse(**converse_api_params)\n",
    "    \n",
    "    print(\"\\nMetrics the for call to the LLM including the internet search text:\")\n",
    "    token_usage = response['usage']\n",
    "    input_tokens = token_usage['inputTokens']\n",
    "    print(f\"Input tokens:  {input_tokens}\")\n",
    "    output_tokens = token_usage['outputTokens']\n",
    "    print(f\"Output tokens:  {output_tokens}\")\n",
    "    total_tokens = token_usage['totalTokens']\n",
    "    print(f\"Total tokens:  {total_tokens}\")\n",
    "    latency = response['metrics']['latencyMs']\n",
    "    print(f\"Latency: {latency} ms\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Which country won the most gold medals in the 2020 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Which country won the most gold medals in the 2024 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is the current weather in Seattle, Wa right now?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is the current price on Amazon stock?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"How many Grizzly bears are living in Washington State?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Searching and scraping with Google search\n",
    "\n",
    "In this example we create an additional function to process the web pages based on URLs returned by the Google search:\n",
    "\n",
    "* get_google_page_content\n",
    "    * This function uses the BeautifulSoup module to parse the html content of a single website URL\n",
    "    * Then the text is processed to remove spaces, blank lines, and short lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the search provider variable\n",
    "search_provider = \"Google\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_google_page_content(url):\n",
    "    try:\n",
    "        # Supply different headers for your requests to avoid bot detection\n",
    "        user_agents = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 17_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/128.0.6613.98 Mobile/15E148 Safari/604.1\",\n",
    "        \"Mozilla/5.0 (iPad; CPU OS 17_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/128.0.6613.98 Mobile/15E148 Safari/604.1\"\n",
    "        ]\n",
    "        user_agent = random.choice(user_agents)\n",
    "        \n",
    "        # Supply common html header elements for Chrome clients\n",
    "        headers = {\n",
    "            \"User-Agent\": user_agent,\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"Sec-Fetch-Dest\": \"document\",\n",
    "            \"Sec-Fetch-Mode\": \"navigate\",\n",
    "            \"Sec-Fetch-Site\": \"none\",\n",
    "            \"Sec-Fetch-User\": \"?1\",\n",
    "            \"Cache-Control\": \"max-age=0\",\n",
    "        }\n",
    "        # Check the URL to see if it is a link to a PDF doc and skip\n",
    "        # This code could be extended to also parse PDF docs rather than skipping\n",
    "        if \".pdf\" in url.split('/')[-1]:\n",
    "            print(f\"Found a PDF file: {url} skipping...\")\n",
    "            return \"skip page\"\n",
    "        else:\n",
    "            # Use the requests module to get the contents of the URL\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "    \n",
    "            if response:\n",
    "                # Parse HTML content\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                # Remove script and style elements\n",
    "                for script_or_style in soup([\"script\", \"style\"]):\n",
    "                    script_or_style.decompose()\n",
    "                # Get the text\n",
    "                text = soup.get_text()\n",
    "                # Break into lines and remove leading and trailing space on each\n",
    "                lines = (line.strip() for line in text.splitlines())\n",
    "                # Break multi-headlines into a line each\n",
    "                chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "                # Drop blank lines\n",
    "                no_blank_lines = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "                # Break into lines again and remove any short lines\n",
    "                lines = no_blank_lines.splitlines()\n",
    "                cleaned_text = \"\"\n",
    "                character_count = 0\n",
    "                for line in lines:\n",
    "                    if len(line) >= 20:\n",
    "                        cleaned_text += line\n",
    "                return cleaned_text\n",
    "            else:\n",
    "                raise Exception(\"No response from the web server.\")\n",
    "    except requests.exceptions.Timeout as timeout_err: \n",
    "        print(f\"Timeout on this URL: {url} skipping...\")\n",
    "        return \"skip page\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error while requesting content from {url} skipping...: {e}\")\n",
    "        return \"skip page\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(\"Who won the 2019 Masters golf tournament?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Who won the 2023 Masters golf tournament?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is the current weather in Seattle, Wa right now?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is the current time and date in Seattle, WA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is the current price on Amazon stock?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(\"Which country won the most gold medals in the 2020 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Which country won the most gold medals in the 2024 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Who is favored to be the next Prime Minister of Canada?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Guardrail\n",
    "Guardrails for Amazon Bedrock have multiple components which include Content Filters, Denied Topics, Word and Phrase Filters, and Sensitive Word (PII & Regex) Filters. For a full list check out the [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-create.html) \n",
    "\n",
    "For our research assistant with web access usecase, we want to prevent inappropriate or malicious questions from being sent to the LLM model as well as preventing our model from returning inappropriate responses or exposing any PII data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the boto3 bedrock client to create a Bedrock Guardrail based on the specific controls we want to enforce\n",
    "create_response = bedrock_admin_client.create_guardrail(\n",
    "    name='research-assistant-guardrail',\n",
    "    description='Prevents inappropriate or malicious questions and model answers. Also blocks political topics and anonymizes PII data.',\n",
    "    topicPolicyConfig={\n",
    "        'topicsConfig': [\n",
    "            {\n",
    "                'name': 'Politics',\n",
    "                'definition': 'Preventing the user from asking questions related to politics for any country.',\n",
    "                'examples': [\n",
    "                    'Who is expected to win the next race for Prime Minister of India?',\n",
    "                    'Which politcial party is in power in England?',\n",
    "                    'Which country has had the most impeachments of heads of state?',\n",
    "                    'Who should I vote for in the next election?',\n",
    "                    'Which countries have had the most political scandals this year?'\n",
    "                ],\n",
    "                'type': 'DENY'\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    contentPolicyConfig={\n",
    "        'filtersConfig': [\n",
    "            {\n",
    "                'type': 'SEXUAL',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'HIGH'\n",
    "            },\n",
    "            {\n",
    "                'type': 'VIOLENCE',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'HIGH'\n",
    "            },\n",
    "            {\n",
    "                'type': 'HATE',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'HIGH'\n",
    "            },\n",
    "            {\n",
    "                'type': 'INSULTS',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'HIGH'\n",
    "            },\n",
    "            {\n",
    "                'type': 'MISCONDUCT',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'HIGH'\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    wordPolicyConfig={\n",
    "        'wordsConfig': [\n",
    "            {'text': 'political party'},\n",
    "            {'text': 'voting for'},\n",
    "            {'text': 'politics'},\n",
    "            {'text': 'voting advice'},\n",
    "            {'text': 'vote for President'},\n",
    "            {'text': 'vote for Prime'},\n",
    "            {'text': 'vote for Chancellor'},\n",
    "            {'text': 'King and Queen'},\n",
    "            {'text': 'Duke and Duchess'},\n",
    "            {'text': 'Chairman of North'},\n",
    "            {'text': 'Supreme Leader'}\n",
    "        ],\n",
    "        'managedWordListsConfig': [\n",
    "            {'type': 'PROFANITY'}\n",
    "        ]\n",
    "    },\n",
    "    sensitiveInformationPolicyConfig={\n",
    "        'piiEntitiesConfig': [\n",
    "            {'type': 'EMAIL', 'action': 'ANONYMIZE'},\n",
    "            {'type': 'PHONE', 'action': 'ANONYMIZE'},\n",
    "            {'type': 'US_SOCIAL_SECURITY_NUMBER', 'action': 'ANONYMIZE'},\n",
    "            {'type': 'US_BANK_ACCOUNT_NUMBER', 'action': 'ANONYMIZE'},\n",
    "            {'type': 'CREDIT_DEBIT_CARD_NUMBER', 'action': 'ANONYMIZE'}\n",
    "        ]\n",
    "    },\n",
    "    blockedInputMessaging=\"\"\"I can provide answers for your research, but I'm not allowed to answer this particular question. Please try a different question. \"\"\",\n",
    "    blockedOutputsMessaging=\"\"\"I'm not allowed to share the answer to this particular question. Please try a different question.\"\"\",\n",
    "    tags=[\n",
    "        {'key': 'purpose', 'value': 'inappropriate-websearch-prevention'},\n",
    "        {'key': 'environment', 'value': 'production'}\n",
    "    ]\n",
    ")\n",
    "\n",
    "pprint.pprint(create_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a versioned snapshot of our draft Guardrail \n",
    "version_response = bedrock_admin_client.create_guardrail_version(\n",
    "    guardrailIdentifier=create_response['guardrailId'],\n",
    "    description='Version of research assistant Guardrail'\n",
    ")\n",
    "pprint.pprint(version_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Guardrail config that we can pass into the Converse API call\n",
    "# Use the Guardrail ID and version that we just created above.\n",
    "# Optionally, enable the Guardrail trace so that we can view the effect it has on questions and answers.\n",
    "guardrail_config = {\n",
    "    \"guardrailIdentifier\": version_response['guardrailId'],\n",
    "    \"guardrailVersion\": version_response['version'],\n",
    "    \"trace\": \"enabled\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modify the function that answers the question based on google search content to use the Guardrail\n",
    "# Add the Guardrail context to the messages array that we use in the converse API call \n",
    "# Add the Guardrail config to the converse API parameters\n",
    "def answer_question_with_content(question, content):\n",
    "    query = f\"\"\"\n",
    "    Based solely on this content:\n",
    "    <content>\n",
    "    {content}\n",
    "    </content>\n",
    "    Answer this question:\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "    Skip any preamble or references to the tool.\n",
    "    \"\"\"\n",
    "\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"messages\":[\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"guardContent\": {\"text\": {\"text\": query}}}]\n",
    "            }\n",
    "        ],\n",
    "        \"system\": [{ \"text\": \"You are an expert research assistant.\"}],\n",
    "        \"inferenceConfig\":{\n",
    "            \"maxTokens\": 4096,\n",
    "            \"temperature\": 0\n",
    "        },\n",
    "        \"guardrailConfig\": guardrail_config,\n",
    "    }\n",
    "\n",
    "    response = bedrock_runtime_client.converse(**converse_api_params)\n",
    "    if response['stopReason'] == \"guardrail_intervened\":\n",
    "            trace = response['trace']\n",
    "            print(\"Guardrail trace:\")\n",
    "            pprint.pprint(trace['guardrail'])\n",
    "            \n",
    "    print(\"\\nMetrics the for call to the LLM including the internet search text:\")\n",
    "    token_usage = response['usage']\n",
    "    input_tokens = token_usage['inputTokens']\n",
    "    print(f\"Input tokens:  {input_tokens}\")\n",
    "    output_tokens = token_usage['outputTokens']\n",
    "    print(f\"Output tokens:  {output_tokens}\")\n",
    "    total_tokens = token_usage['totalTokens']\n",
    "    print(f\"Total tokens:  {total_tokens}\")\n",
    "    latency = response['metrics']['latencyMs']\n",
    "    print(f\"Latency: {latency} ms\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modify the function that answers the question directly or outputs tool use if an internet search is required\n",
    "# Add the Guardrail context to the messages array that we use in the converse API call \n",
    "# Add the Guardrail config to the converse API parameters\n",
    "def answer_question(question):\n",
    "    query = f\"\"\"\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    You have access to the internet_search tool.\n",
    "    Take your time to first think step by step if the question could be answered within your own knowledge.\n",
    "    Output your thinking in <thinking></thinking> tags\n",
    "    Only use the internet_search tool as a last resort if you cannot answer the user's question within the question tags from your own knowledge.\n",
    "    For example, only use the internet_search_tool if the subject, product, or event is more recent than your training cutoff date.\n",
    "    \"\"\"\n",
    "\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"messages\":[\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"guardContent\": {\"text\": {\"text\": query}}}]\n",
    "            }\n",
    "        ],\n",
    "        \"toolConfig\": toolConfig,\n",
    "        \"system\": [{ \"text\": \"You are an expert research assistant.\"}],\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": 4096,\n",
    "            \"temperature\": 0\n",
    "        },\n",
    "        \"guardrailConfig\": guardrail_config,\n",
    "    }\n",
    "\n",
    "    response = bedrock_runtime_client.converse(**converse_api_params)\n",
    "    print(\"Metrics the for first call to the LLM:\")\n",
    "    token_usage = response['usage']\n",
    "    input_tokens = token_usage['inputTokens']\n",
    "    print(f\"Input tokens:  {input_tokens}\")\n",
    "    output_tokens = token_usage['outputTokens']\n",
    "    print(f\"Output tokens:  {output_tokens}\")\n",
    "    total_tokens = token_usage['totalTokens']\n",
    "    print(f\"Total tokens:  {total_tokens}\")\n",
    "    latency = response['metrics']['latencyMs']\n",
    "    print(f\"Latency: {latency} ms\\n\")\n",
    "    \n",
    "    if response['stopReason'] == \"guardrail_intervened\":\n",
    "            trace = response['trace']\n",
    "            print(\"Guardrail trace:\")\n",
    "            pprint.pprint(trace['guardrail'])\n",
    "\n",
    "\n",
    "    internet_search = None\n",
    "    for content in response['output']['message']['content']:\n",
    "        if isinstance(content, dict) and 'toolUse' in content:\n",
    "            tool_use = content['toolUse']\n",
    "            if tool_use['name'] == \"internet_search\":\n",
    "                internet_search = tool_use['input']\n",
    "                break\n",
    "\n",
    "    if internet_search:\n",
    "        question = internet_search[\"question\"]\n",
    "        content = handle_search(question, search_provider)\n",
    "        if content:\n",
    "            print(\"\\nInternet search successful\")\n",
    "            response = answer_question_with_content(question, content)\n",
    "            print(f\"\\nFinal answer = {response['output']['message']['content'][-1]['text']}\\n\")\n",
    "        else:\n",
    "            print(\"No content found from Internet search\")\n",
    "    else:\n",
    "        print(\"No Internet search needed.\")\n",
    "        print(f\"\\nFinal answer is: {response['output']['message']['content'][-1]['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Who won the 2023 Masters golf tournament?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Who is favored to win the next election for Prime Minister of Canada?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is the email address for AWS Support?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Provide me a social security number of a dead person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Where can I purchace brass knuckles?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"How many Grizzly bears are living in Washington State?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (when running from your own AWS account)\n",
    "\n",
    "You only need to clean up if running this workshop from your own AWS account. \n",
    "If you are running from an AWS-facilitated event, this will be done automatically for you.\n",
    "\n",
    "After completing the workshop, follow these steps to clean up your AWS environment and avoid unnecessary charges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the Guardrail by specifying the Guardrail arn\n",
    "delete_guardrail_response = bedrock_admin_client.delete_guardrail(\n",
    "    guardrailIdentifier=create_response['guardrailArn']\n",
    ")\n",
    "pprint.pprint(delete_guardrail_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List the Guardrails to ensure that the research-assistant-guardrail is deleted\n",
    "list_guardrails_response = bedrock_admin_client.list_guardrails()\n",
    "pprint.pprint(list_guardrails_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
