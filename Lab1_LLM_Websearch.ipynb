{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Web Search\n",
    "\n",
    "> *This notebook should work well with the **`conda_python3`** kernel in SageMaker Studio*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we show you how to:\n",
    "- Define a tool that the LLM can reliably call that produces JSON output\n",
    "- Use the googlesearch and wikipedia python modules to search the internet if the LLM cannot answer a research question itself\n",
    "- Rerank the search results options from best to worst\n",
    "- Scrape and process the best option HTML page to create context for the LLM\n",
    "\n",
    "We will use Bedrock's `Claude 3 Sonnet`, `Claude 3.5 Sonnet`(default), and `Claude 3 Haiku` base model using the AWS boto3 SDK. \n",
    "\n",
    "> **Note:** *This notebook can be used in SageMaker Studio or run locally if you setup your AWS credentials.*\n",
    "\n",
    "#### Prerequisites\n",
    "- This notebook requires permissions to access Amazon Bedrock\n",
    "- Ensure you have gone to the Bedrock models access page in the AWS Console and enabled access to `Anthropic Claude 3 Sonnet`\n",
    "- If you are running this notebook without an Admin role, make sure that your notebook's role includes the following managed policy:\n",
    "> AmazonBedrockFullAccess\n",
    "\n",
    "#### Use case\n",
    "You are building a research assistant GenAI application. In some cases the user's question may be about an event, product, or service that is more recent than the cutoff training date for the LLM model or not within the model's knowledge. For these cases, we want the LLM model to call the internet search tool to gather context relating to the question. Then we can supply that context back to the LLM to answer the question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook setup\n",
    "\n",
    "1. If you are attending an instructor lead workshop or deployed the workshop infrastructure using the provided [CloudFormation Template](https://raw.githubusercontent.com/aws-samples/xxx/main/cloudformation/workshop-v1-final-cfn.yml) you can proceed to step 2, otherwise you will need to download the workshop [GitHub Repository](https://github.com/aws-samples/xxx) to your local machine.\n",
    "\n",
    "2. Install the required dependencies by running the pip install commands in the next cell.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "‚ö†Ô∏è **Please ignore error messages related to pip's dependency resolver.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip** You can use `Shift + Enter` to execute the cell and move to the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU pip\n",
    "!pip install -r requirements.txt\n",
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import string\n",
    "import pprint\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "from googlesearch import search\n",
    "import wikipedia\n",
    "from bs4 import BeautifulSoup\n",
    "from botocore.exceptions import ClientError\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "session = boto3.Session()\n",
    "region = session.region_name\n",
    "\n",
    "# Change which line is uncommented below to select the LLM model you want to use\n",
    "#modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "#modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "modelId = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "\n",
    "print(f\"Using modelId: {modelId}\")\n",
    "print(f\"Using region: {region}\")\n",
    "print('Running boto3 version:', boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `modelId` and `region` variables defined in the above cell will be used throughout the workshop.\n",
    "\n",
    "Just make sure to run the cells from top to bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Boto3 SDK & the Converse API\n",
    "We will be using the [Amazon Boto3 SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime.html) and the [Converse API](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html) throughout this workshop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a boto3 Bedrock runtime client for calling the LLM\n",
    "bedrock_runtime_client = boto3.client(service_name = 'bedrock-runtime', region_name = region,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking the LLM questions without an internet search tool\n",
    "\n",
    "Let's start out by asking some questions to the LLM without supplying the option of an internet search tool.\n",
    "\n",
    "Create two functions:\n",
    "* call_bedrock\n",
    "    * This function takes in the parmeters you set for the Bedrock converse API and uses the runtime client to make the call to Bedrock converse API\n",
    "    * A retry with backoff mechanism is put in place to catch any throttling response from Bedrock\n",
    "* answer_question\n",
    "    * This function defines the prompt template and the converse api parameters\n",
    "    * It passes that to the call_bedrock function and then parses the response for printing out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for calling the Bedrock Converse API...\n",
    "def call_bedrock(messages, system_prompt, tool_config, tries=0):\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"system\": [{ \"text\": system_prompt}],\n",
    "        \"messages\": messages,\n",
    "        \"toolConfig\": tool_config,\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": 4096,\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    }\n",
    "    # Remove tool config if not using this\n",
    "    if tool_config:\n",
    "        pass\n",
    "    else:\n",
    "        del converse_api_params[\"toolConfig\"]\n",
    "    \n",
    "    # Loop and retry the Bedrock call in case a throttling exception is returned\n",
    "    while tries <= 3:\n",
    "        tries += 1\n",
    "        try:\n",
    "            # Call the LLM model via the Converse API\n",
    "            response = bedrock_runtime_client.converse(**converse_api_params)\n",
    "        except ClientError as err:\n",
    "            # Handle the throttling error and retry the call to Bedrock\n",
    "            if err.response['Error']['Code'] == 'ThrottlingException':\n",
    "                if tries <=3:\n",
    "                    print(\"Throttling Exception Occured...Retrying...\")\n",
    "                    print(\"Attempt No.: \" + str(tries))\n",
    "                    time.sleep(5*tries)\n",
    "                    continue\n",
    "                else:\n",
    "                    raise Exception (\"Attempted 3 Times But No Success.\")\n",
    "                    return False\n",
    "            else:\n",
    "                print(f\"Bedrock Client Error: {err}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as err:\n",
    "            print(f\"Error while calling the Bedrock API: {err}\")\n",
    "            return False\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    query = f\"\"\"\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    Answer the user's question in complete sentances.\n",
    "    Skip the preamble.\n",
    "    \n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": query}]}]\n",
    "    system_prompt = \"You are an expert research assistant.\"\n",
    "    \n",
    "    response = call_bedrock(messages, system_prompt, tool_config={})\n",
    "    if response:\n",
    "        # Provide the LLM's response\n",
    "        print(f\"\\nFinal answer is: {response['output']['message']['content'][-1]['text']}\\n\")\n",
    "    else:\n",
    "        print(\"Unable to get a response from Bedrock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The LLM should be able to answer this question from it's own knowledge\n",
    "answer_question(\"Which country won the most gold medals in the 2020 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now try a question where the information is too new and past the LLM's training cutoff date\n",
    "answer_question(\"Which country won the most gold medals in the 2024 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is the current weather in Seattle, Wa right now?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Searching and scraping with Wikipedia and Google\n",
    "\n",
    "In this example we create two functions:\n",
    "* internet_search\n",
    "    * This function first calls the internet provider to search for pages (Wikipedia) / URLs (Google) related to the user's question\n",
    "    * Use the `num_results` parameter to control how many pages/URLs you want returned\n",
    "    * Then it returns the list of pages/URLs\n",
    "* get_wikipedia_page_content\n",
    "    * This function uses the wikipedia module to get the html content of a single Wikipedia page\n",
    "    * The markdownify module is used to transform the page markdown (including tables) to lines of text\n",
    "    * Then the text is processed to remove the standard info sections at the bottom of Wikipedia pages from the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Websearch provider\n",
    "search_provider = \"Wikipedia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToolsList:\n",
    "    def internet_search(self, question):\n",
    "        num_results = 5\n",
    "        # Proceed with internet search\n",
    "        print(f\"Searching {search_provider}...\\n\")\n",
    "        try:\n",
    "            if search_provider == \"Wikipedia\":\n",
    "                # Use the wikipedia module to get wiki pages related to the user's question\n",
    "                search_results = wikipedia.search(question, results=num_results)\n",
    "            elif search_provider == \"Google\":\n",
    "                # Sometimes Google will only return one page even if asked for more, try again if only one\n",
    "                search_results = ['dummy']\n",
    "                while len(search_results) == 1:\n",
    "                    # Use the googlesearch module to get pages related to the user's question\n",
    "                    for page in search(question, sleep_interval=5, num_results=num_results):\n",
    "                        search_results.append(page)\n",
    "                    if len(search_results) != 1:\n",
    "                        break\n",
    "            # Return the list of pages/URLs returned by the internet search provider\n",
    "            return search_results\n",
    "        except Exception as err:\n",
    "            print(f\"Error during internet_search: {err}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_wikipedia_page_content(page):\n",
    "    try:\n",
    "        # use the wikipedia module to return the html content of a specific page\n",
    "        html_text = wikipedia.page(title=page, auto_suggest=False).html()\n",
    "        # As a first cleanup step and to retain tabular data, we will use the markdownify module to convert html to markdown text\n",
    "        markdown_text = md(html_text)\n",
    "        payload = \"{} \\n\".format(markdown_text)\n",
    "        # Initialize a variable to hold our cleaned markdown text as we process each line\n",
    "        cleaned_markdown_text = \"\"\n",
    "        if markdown_text:\n",
    "            lines = payload.splitlines()\n",
    "            # Do not include the standard info sections at the bottom of Wikipedia pages in the content\n",
    "            # Do not include the edit links or links to images to reduce token count\n",
    "            for line in lines:\n",
    "                if line == \"\" or \"[edit]\" in line or \"[![]\" in line:\n",
    "                    continue\n",
    "                elif line == \"See also\" or line == \"References\" or line == \"External link\" or line == \"Further reading\":\n",
    "                    break\n",
    "                else:\n",
    "                    # Only add lines that contain the main content of the page\n",
    "                    cleaned_markdown_text += line + '\\n'\n",
    "        else:\n",
    "            # Handle the situation where no content was found on a page\n",
    "            Print(f\"No markdown text found on page: {page}\")\n",
    "    except Exception as err:\n",
    "        print(f\"Error while requesting content from {page} skipping...: {err}\")\n",
    "        return \"skip page\"\n",
    "    \n",
    "    return cleaned_markdown_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use the Bedrock Converse API for inference and configure 'Tool Use'\n",
    "\n",
    "* Configure the tool definition\n",
    "    * This JSON schema defines our internet search tool and how the LLM should output the JSON when calling the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tool definition\n",
    "provider_websearch_schema = {\n",
    "      \"toolSpec\": {\n",
    "        \"name\": \"internet_search\",\n",
    "        \"description\": \"A tool to retrieve up to date information from an internet search.\",\n",
    "        \"inputSchema\": {\n",
    "          \"json\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The users question as-is for the internet search.\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"question\"]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "# In this example, we save only one tool schema to the configuration, but you could have many tools\n",
    "tool_config = {\n",
    "    \"tools\": [provider_websearch_schema],\n",
    "    \"toolChoice\": {\"auto\": {}}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the answer_question function\n",
    "This is the main function for orchestrating the entire conversation flow\n",
    "\n",
    "* This function calls the LLM to answer the user's question directly or outputs 'tool use' JSON if an internet search is required\n",
    "* Note that the LLM will have a propencity to use the tool, so we must direct it in the prompt to only do so as a last resort\n",
    "* If the LLM decides it needs to use the tool, it will output the tool name and arguments in JSON format\n",
    "* Then the tool is invoked and provided the tool arguments which produces a list of Wikipedia pages or Google URLs depending on which search provider is defined\n",
    "* The list of pages/URLs is sent to the same model to rerank them in the order of best option to worst option\n",
    "* The reranked options are iterated through until a valid response is returned. We only want one valid response to save on cost and reduce the token count we send the LLM.\n",
    "* Finally, we send the original user's question along with the content scraped from the Wikipedia page or Google URL to the LLM to arrive at a final answer\n",
    "\n",
    "Note: As we progress through the requests and responses, we will add them to a messages_trace. If you want to see the entire conversation, you can uncomment the print statement at the bottom of the function to print out the entire message_trace, run the answer_question cell again, and ask your questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function for orchestrating the conversation flow...\n",
    "\n",
    "def answer_question(question):\n",
    "    # Initialize the messages_trace array:\n",
    "    messages_trace = []\n",
    "    # Create the initial message including the user's question\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": question}]}]\n",
    "    # Append this message to the messages_trace\n",
    "    messages_trace.append(messages)\n",
    "    \n",
    "    system_prompt = f\"\"\"\n",
    "    Only search the web for queries that you can not confidently answer.\n",
    "    Today's date is {dt.now().strftime(\"%B %d %Y\")}\n",
    "    If you think a user's question involves something in the future that hasn't happened yet, use the search tool.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = call_bedrock(messages, system_prompt, tool_config)\n",
    "    if response:\n",
    "        # Check the LLM's response to see if it answered the question or needs to use the tool\n",
    "        use_tool = None\n",
    "        for content in response['output']['message']['content']:\n",
    "            if isinstance(content, dict) and 'toolUse' in content:\n",
    "                tool_use = content['toolUse']\n",
    "                if tool_use['name'] == \"internet_search\":\n",
    "                    use_tool = tool_use['input']\n",
    "                    break\n",
    "\n",
    "        #Add the intermediate output to the messages_trace array:\n",
    "        messages_trace.append(response['output']['message'])\n",
    "        \n",
    "        if use_tool:            \n",
    "            # Get the tool name and arguments:\n",
    "            tool_name = tool_use['name']\n",
    "            print(f\"Calling tool: {tool_name}\")\n",
    "            tool_args = tool_use['input'] or {}\n",
    "            print(f\"Tool args are: {tool_args}\")\n",
    "    \n",
    "            # Call the tool function:\n",
    "            tool_response = getattr(ToolsList(), tool_name)(**tool_args) or \"\"\n",
    "            if tool_response:\n",
    "                tool_status = 'success'\n",
    "            else:\n",
    "                tool_status = 'error'\n",
    "            print(f\"Tool response is: {tool_response}\")\n",
    "            tool_response = json.dumps(tool_response)\n",
    "            #Add the tool result to the messages_trace:\n",
    "            messages_trace.append(\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            'toolResult': {\n",
    "                                'toolUseId':tool_use['toolUseId'],\n",
    "                                'content': [\n",
    "                                    {\n",
    "                                        \"text\": tool_response\n",
    "                                    }\n",
    "                                ],\n",
    "                                'status': tool_status\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # RERANK\n",
    "            # We want to avoid having to send the contents for all pages/URLs returned by the internet search tool as that would be more expensive\n",
    "            # So we will call the same modelId we have specified initially and pass it the list of pages/URLs\n",
    "            # We will ask the model to rerank the list in the order of best option to worst option\n",
    "            # Then we will scrape the page of only the best option to provide up-to-date context related to the user's question\n",
    "            query = f\"\"\"\n",
    "            Given this user's question:\n",
    "            <question>\n",
    "            {question}\n",
    "            </question>\n",
    "\n",
    "            Rank from best to worst the choices that are provided in the choices tags for searching the internet to provide an answer to the user's question.\n",
    "            <choices>\n",
    "            {tool_response}\n",
    "            </choices>\n",
    "            Skip the preamble and do not include any reasoning in your output.\n",
    "            Do not enumerate or add anything to the list.\n",
    "            Simply return the choices in a JSON list from best to worst choice.\n",
    "            \"\"\"\n",
    "            messages = [{\"role\": \"user\", \"content\": [{\"text\": query}]}]\n",
    "            # Append this message to our messages_trace\n",
    "            messages_trace.append(messages)\n",
    "            system_prompt = \"You are an expert research assistant.\"\n",
    "            \n",
    "            # Call the LLM to rerank the pages/URLs from best to worst based on the user's question\n",
    "            response = call_bedrock(messages, system_prompt, tool_config={})\n",
    "            if response:\n",
    "                reranked_options = response['output']['message']['content'][-1]['text']\n",
    "                reranked_options = json.loads(reranked_options)\n",
    "            else:\n",
    "                print(\"Unable to get a response from Bedrock at the reranking step\")\n",
    "                return False\n",
    "            print(f\"reranked_options are: {reranked_options}\")\n",
    "            messages_trace.append(response['output']['message'])\n",
    "            \n",
    "            for option in reranked_options:\n",
    "                print(f\"\\nScraping page: {option}\")\n",
    "                if search_provider == \"Wikipedia\":\n",
    "                    content = get_wikipedia_page_content(option)\n",
    "                elif search_provider == \"Google\":\n",
    "                    content = get_google_page_content(option)\n",
    "\n",
    "                if content and content != \"skip page\":\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "        \n",
    "            # FINAL REQUEST\n",
    "            #Invoke the model one more time and provide it with the content gathered from the internet\n",
    "            query = f\"\"\"\n",
    "            Based solely on this content:\n",
    "            <content>\n",
    "            {content}\n",
    "            </content>\n",
    "            Answer this question:\n",
    "            <question>\n",
    "            {question}\n",
    "            </question>\n",
    "            Skip any preamble or references to the tool.\n",
    "            \"\"\"\n",
    "            messages = [{\"role\": \"user\", \"content\": [{\"text\": query}]}]\n",
    "            messages_trace.append(messages)\n",
    "            system_prompt = \"Answer the user's question based on what was returned by the tool\"\n",
    "            response = call_bedrock(messages, system_prompt, tool_config={})\n",
    "            \n",
    "            #Add the final response to the messages array:\n",
    "            messages_trace.append(response['output']['message'])\n",
    "            # Uncomment the line below to print out the entire request - response trace of messages\n",
    "            #print(f\"All queries and responses:\\n{json.dumps(messages_trace, indent=2)}\")\n",
    "            \n",
    "            print(f\"\\nFinal answer:\\n{response['output']['message']['content'][-1]['text']}\")\n",
    "            \n",
    "        else:\n",
    "            # Uncomment the line below to print out the entire request - response trace of messages\n",
    "            #print(f\"All queries and responses:\\n{json.dumps(messages_trace, indent=2)}\")\n",
    "            print(\"No need to call the internet search tool\")\n",
    "            print(f\"\\nFinal answer:\\n{response['output']['message']['content'][-1]['text']}\")\n",
    "    else:\n",
    "        print(\"No response returned from the LLM\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Why is the sky blue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Which country won the most gold medals in the 2020 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Which country won the most gold medals in the 2024 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is the current weather in Seattle, Wa right now?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"How many Grizzly bears are living in Washington State?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Searching and scraping with Google search\n",
    "\n",
    "In this example we create an additional function to process the web pages based on URLs returned by the Google search:\n",
    "\n",
    "* get_google_page_content\n",
    "    * This function uses the BeautifulSoup module to parse the html content of a single website URL\n",
    "    * Then the text is processed to remove spaces, blank lines, and short lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_google_page_content(url):\n",
    "    try:\n",
    "        # TODO: remove and will replace with Google API client \n",
    "        user_agents = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 17_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/128.0.6613.98 Mobile/15E148 Safari/604.1\",\n",
    "        \"Mozilla/5.0 (iPad; CPU OS 17_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/128.0.6613.98 Mobile/15E148 Safari/604.1\"\n",
    "        ]\n",
    "        user_agent = random.choice(user_agents)\n",
    "        \n",
    "        # Supply common html header elements for Chrome clients\n",
    "        headers = {\n",
    "            \"User-Agent\": user_agent,\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"Sec-Fetch-Dest\": \"document\",\n",
    "            \"Sec-Fetch-Mode\": \"navigate\",\n",
    "            \"Sec-Fetch-Site\": \"none\",\n",
    "            \"Sec-Fetch-User\": \"?1\",\n",
    "            \"Cache-Control\": \"max-age=0\",\n",
    "        }\n",
    "        # Check the URL to see if it is a link to a PDF doc and skip\n",
    "        # This code could be extended to also parse PDF docs rather than skipping\n",
    "        if \".pdf\" in url.split('/')[-1]:\n",
    "            print(f\"Found a PDF file: {url} skipping...\")\n",
    "            return \"skip page\"\n",
    "        else:\n",
    "            # Use the requests module to get the contents of the URL\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "    \n",
    "            if response:\n",
    "                # Parse HTML content\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                # Remove script and style elements\n",
    "                for script_or_style in soup([\"script\", \"style\"]):\n",
    "                    script_or_style.decompose()\n",
    "                # Get the text\n",
    "                text = soup.get_text()\n",
    "                # Break into lines and remove leading and trailing space on each\n",
    "                lines = (line.strip() for line in text.splitlines())\n",
    "                # Break multi-headlines into a line each\n",
    "                chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "                # Drop blank lines\n",
    "                no_blank_lines = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "                # Break into lines again and remove any short lines\n",
    "                lines = no_blank_lines.splitlines()\n",
    "                cleaned_text = \"\"\n",
    "                character_count = 0\n",
    "                for line in lines:\n",
    "                    if len(line) >= 20:\n",
    "                        cleaned_text += line\n",
    "                return cleaned_text\n",
    "            else:\n",
    "                raise Exception(\"No response from the web server.\")\n",
    "    except requests.exceptions.Timeout as timeout_err: \n",
    "        print(f\"Timeout on this URL: {url} skipping...\")\n",
    "        return \"skip page\"\n",
    "    except Exception as err:\n",
    "        print(f\"Error while requesting content from {url} skipping...: {err}\")\n",
    "        return \"skip page\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Websearch provider\n",
    "search_provider = \"Google\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Who won the 2019 Masters golf tournament?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Which country won the most gold medals in the 2020 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Which country won the most gold medals in the 2024 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is the current weather in Seattle, Wa right now?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is the current price on Amazon stock?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Who is favored to be the next Prime Minister of Canada?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
