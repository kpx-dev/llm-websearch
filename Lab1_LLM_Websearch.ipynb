{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Web Search\n",
    "\n",
    "> *This notebook should work well with the **`conda_python3`** kernel in SageMaker Studio*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we show you how to:\n",
    "- Define a tool that the LLM can reliably call that produces JSON output\n",
    "- Use the googlesearch and wikipedia python modules to search the internet if the LLM cannot answer a research question itself\n",
    "- Rerank the search results options from best to worst\n",
    "- Scrape and process the best option HTML page to create context for the LLM\n",
    "\n",
    "We will use Bedrock's `Claude 3 Sonnet` base model using the AWS boto3 SDK. \n",
    "\n",
    "> **Note:** *This notebook can be used in SageMaker Studio or run locally if you setup your AWS credentials.*\n",
    "\n",
    "#### Prerequisites\n",
    "- This notebook requires permissions to access Amazon Bedrock\n",
    "- Ensure you have gone to the Bedrock models access page in the AWS Console and enabled access to `Anthropic Claude 3 Sonnet`\n",
    "- If you are running this notebook without an Admin role, make sure that your notebook's role includes the following managed policy:\n",
    "> AmazonBedrockFullAccess\n",
    "\n",
    "#### Use case\n",
    "You are building a research assistant GenAI application. In some cases the user's question may be about an event, product, or service that is more recent than the cutoff training date for the LLM model or not within the model's knowledge. For these cases, we want the LLM model to call the internet search tool to gather context relating to the question. Then we can supply that context back to the LLM to answer the question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notebook setup\n",
    "\n",
    "1. If you are attending an instructor lead workshop or deployed the workshop infrastructure using the provided [CloudFormation Template](https://raw.githubusercontent.com/aws-samples/xxx/main/cloudformation/workshop-v1-final-cfn.yml) you can proceed to step 2, otherwise you will need to download the workshop [GitHub Repository](https://github.com/aws-samples/xxx) to your local machine.\n",
    "\n",
    "2. Install the required dependencies by running the pip install commands in the next cell.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "‚ö†Ô∏è **Please ignore error messages related to pip's dependency resolver.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip** You can use `Shift + Enter` to execute the cell and move to the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -qU pip\n",
    "!pip install -qU virtualenv\n",
    "!virtualenv env\n",
    "!source env/bin/activate\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import string\n",
    "import pprint\n",
    "import random\n",
    "import time\n",
    "from googlesearch import search\n",
    "import wikipedia\n",
    "from bs4 import BeautifulSoup\n",
    "from botocore.exceptions import ClientError\n",
    "from markdownify import markdownify as md\n",
    "\n",
    "session = boto3.Session()\n",
    "region = session.region_name\n",
    "\n",
    "# Change which line is uncommented below to select the LLM model you want to use\n",
    "modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "#modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "#modelId = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "\n",
    "print(f\"Using modelId: {modelId}\")\n",
    "print(f\"Using region: {region}\")\n",
    "print('Running boto3 version:', boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `modelId` and `region` variables defined in the above cell will be used throughout the workshop.\n",
    "\n",
    "Just make sure to run the cells from top to bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Boto3 SDK & the Converse API\n",
    "We will be using the [Amazon Boto3 SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime.html) and the [Converse API](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html) throughout this workshop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a boto3 Bedrock runtime client for calling the LLM\n",
    "bedrock_runtime_client = boto3.client(service_name = 'bedrock-runtime', region_name = region,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking the LLM questions without an internet search tool\n",
    "\n",
    "Let's start out by asking some questions to the LLM without supplying the option of an internet search tool.\n",
    "\n",
    "Create two functions:\n",
    "* call_bedrock\n",
    "    * This function takes in the parmeters you set for the Bedrock converse API and uses the runtime client to make the call to Bedrock converse API\n",
    "    * A retry with backoff mechanism is put in place to catch any throttling response from Bedrock\n",
    "* answer_question\n",
    "    * This function defines the prompt template and the converse api parameters\n",
    "    * It passes that to the call_bedrock function and then parses the response for printing out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_bedrock(converse_api_params):\n",
    "    tries=0\n",
    "    while tries <= 3:\n",
    "        tries += 1\n",
    "        try:\n",
    "            response = bedrock_runtime_client.converse(**converse_api_params)\n",
    "        except ClientError as err:\n",
    "            if err.response['Error']['Code'] == 'ThrottlingException':\n",
    "                if tries <=3:\n",
    "                    print(\"Throttling Exception Occured...Retrying...\")\n",
    "                    print(\"Attempt No.: \" + str(tries))\n",
    "                    time.sleep(5*tries)\n",
    "                    continue\n",
    "                else:\n",
    "                    raise Exception (\"Attempted 3 Times But No Success.\")\n",
    "                    return False\n",
    "            else:\n",
    "                print(f\"Bedrock Client Error: {err}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as err:\n",
    "            print(f\"Error while calling the Bedrock API: {err}\")\n",
    "            return False\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    query = f\"\"\"\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    Answer the user's question in complete sentances.\n",
    "    Skip the preamble.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": query}]}],\n",
    "        \"system\": [{ \"text\": \"You are an expert research assistant.\"}],\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": 4096,\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    }\n",
    "    response = call_bedrock(converse_api_params)\n",
    "    if response:\n",
    "        # Provide the LLM's response\n",
    "        print(f\"\\nFinal answer is: {response['output']['message']['content'][-1]['text']}\\n\")\n",
    "    else:\n",
    "        print(\"Unable to get a response from Bedrock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The LLM should be able to answer this question from it's own knowledge\n",
    "answer_question(\"Which country won the most gold medals in the 2020 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now try a question where the information is too new and past the LLM's training cutoff date\n",
    "answer_question(\"Which country won the most gold medals in the 2024 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is the current weather in Seattle, Wa right now?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Searching and scraping with Wikipedia and Google\n",
    "\n",
    "In this example we create three functions:\n",
    "* handle_search\n",
    "    * This function first calls the internet provider to search for pages (Wikipedia) / URLs (Google) related to the user's question\n",
    "    * Use the `num_results` parameter to control how many pages/URLs you want returned\n",
    "    * Then it passes the list of pages/URLs to the reranker function to use the LLM to order the list pages/URLs from best choice to worst choice\n",
    "    * Finally it iterates through the list of pages/URLs to make sure content is there and passes the first (best) choice text block to the LLM as context in the prompt\n",
    "* get_wikipedia_page_content\n",
    "    * This function uses the wikipedia module to get the html content of a single Wikipedia page\n",
    "    * The markdownify module is used to transform the page markdown (including tables) to lines of text\n",
    "    * Then the text is processed to remove the standard info sections at the bottom of Wikipedia pages from the content\n",
    "* reranker\n",
    "    * This function takes the list of pages that the internet search provider returns and uses the LLM to rank them in order from best choice to worst choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the search provider variable\n",
    "search_provider = \"Wikipedia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_search(query, search_provider):\n",
    "    num_results = 5\n",
    "    # Proceed with Wikipedia search\n",
    "    print(f\"Searching {search_provider}...\\n\")\n",
    "    try:\n",
    "        if search_provider == \"Wikipedia\":\n",
    "            # Use the wikipedia module to get wiki pages related to the user's question\n",
    "            search_results = wikipedia.search(query, results=num_results)\n",
    "        elif search_provider == \"Google\":\n",
    "            # Sometimes Google will only return one page even if asked for more, try again if only one\n",
    "            search_results = ['dummy']\n",
    "            while len(search_results) == 1:\n",
    "                # Use the googlesearch module to get pages related to the user's question\n",
    "                for page in search(query, sleep_interval=5, num_results=num_results):\n",
    "                    search_results.append(page)\n",
    "                if len(search_results) != 1:\n",
    "                    break\n",
    "        # Pass the list of pages/URLs to the reranker function\n",
    "        best_options = reranker(query, search_results)\n",
    "        if best_options:\n",
    "            for option in best_options:\n",
    "                print(f\"\\nScraping page: {option}\")\n",
    "                if search_provider == \"Wikipedia\":\n",
    "                    content = get_wikipedia_page_content(option)\n",
    "                elif search_provider == \"Google\":\n",
    "                    content = get_google_page_content(option)\n",
    "\n",
    "                if content and content != \"skip page\":\n",
    "                    return content\n",
    "                else:\n",
    "                    continue\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as err:\n",
    "        print(f\"Error during handle search: {err}\")\n",
    "        return False\n",
    "    return \"No content found on any of the pages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reranker(question, internet_search_results):\n",
    "    query = f\"\"\"\n",
    "    Given this user's question:\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    Rank from best to worst the choices that are provided in the choices tags for searching the internet to provide an answer to the user's question.\n",
    "    <choices>\n",
    "    {internet_search_results}\n",
    "    </choices>\n",
    "    Skip the preamble and do not include any reasoning in your output. \n",
    "    Do not enumerate or add anything to the list.\n",
    "    Simply return the choices in a JSON list from best to worst choice.\n",
    "    \"\"\"\n",
    "\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": query}]}],\n",
    "        \"system\": [{ \"text\": \"You are an expert research assistant.\"}],\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": 4096,\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = call_bedrock(converse_api_params)\n",
    "    if response:\n",
    "        best_options = response['output']['message']['content'][-1]['text']\n",
    "    else:\n",
    "        print(\"Unable to get a response from Bedrock\")\n",
    "        return False\n",
    "    \n",
    "    print(\"Metrics the for reranker call to the LLM:\")\n",
    "    token_usage = response['usage']\n",
    "    input_tokens = token_usage['inputTokens']\n",
    "    print(f\"Input tokens:  {input_tokens}\")\n",
    "    output_tokens = token_usage['outputTokens']\n",
    "    print(f\"Output tokens:  {output_tokens}\")\n",
    "    total_tokens = token_usage['totalTokens']\n",
    "    print(f\"Total tokens:  {total_tokens}\")\n",
    "    latency = response['metrics']['latencyMs']\n",
    "    print(f\"Latency: {latency} ms\\n\")\n",
    "\n",
    "    # Provide the LLM's response\n",
    "    print(f\"\\nThe reranked order of:\\n{internet_search_results} is:\\n{best_options}\\n\")\n",
    "    return json.loads(best_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_wikipedia_page_content(page):\n",
    "    try:\n",
    "        html_text = wikipedia.page(title=page, auto_suggest=False).html()\n",
    "        markdown_text = md(html_text)\n",
    "        payload = \"{} \\n\".format(markdown_text)\n",
    "        cleaned_markdown_text = \"\"\n",
    "        if markdown_text:\n",
    "            lines = payload.splitlines()\n",
    "            # Do not include the standard info sections at the bottom of Wikipedia pages in the content\n",
    "            # Do not include the edit links or links to images to reduce token count\n",
    "            for line in lines:\n",
    "                if line == \"\" or \"[edit]\" in line or \"[![]\" in line:\n",
    "                    continue\n",
    "                elif line == \"See also\" or line == \"References\" or line == \"External link\" or line == \"Further reading\":\n",
    "                    break\n",
    "                else:\n",
    "                    cleaned_markdown_text += line + '\\n'\n",
    "        else:\n",
    "            Print(f\"No markdown text found on page: {page}\")\n",
    "    except Exception as err:\n",
    "        print(f\"Error while requesting content from {page} skipping...: {err}\")\n",
    "        return \"skip page\"\n",
    "    \n",
    "    return cleaned_markdown_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use the Bedrock Converse API for inference and configure 'Tool Use'\n",
    "\n",
    "* Configure the tool definition\n",
    "    * This JSON schema defines our internet search tool and how the LLM should output the JSON when calling the tool\n",
    "* answer_question\n",
    "    * This function calls the LLM to answer the user's question directly or outputs 'tool use' JSON if an internet search is required\n",
    "    * Note that the LLM will have a propencity to use the tool, so we must direct it in the prompt to only do so as a last resort\n",
    "* answer_question_with_content\n",
    "    * This function answers the user's question based on the block of text supplied by the internet provider search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tool definition\n",
    "provider_websearch_schema = {\n",
    "      \"toolSpec\": {\n",
    "        \"name\": \"internet_search\",\n",
    "        \"description\": \"A tool to retrieve up to date information from an internet search.\",\n",
    "        \"inputSchema\": {\n",
    "          \"json\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The users question as-is for the internet search.\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"question\"]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "# In this example, we save only one tool schema to the configuration, but you could have many tools\n",
    "toolConfig = {\n",
    "    \"tools\": [provider_websearch_schema],\n",
    "    \"toolChoice\": {\"auto\": {}}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    query = f\"\"\"\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "\n",
    "    You have access to the internet_search tool.\n",
    "    Take your time to first think step by step if the question could be answered within your own knowledge.\n",
    "    Output your thinking in <thinking></thinking> tags\n",
    "    If you are confident that you can answer the user's question without using the tool, then respond normally.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": query}]}],\n",
    "        \"toolConfig\": toolConfig,\n",
    "        \"system\": [{ \"text\": \"You are an expert research assistant\"}],\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": 4096,\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    }\n",
    "    response = call_bedrock(converse_api_params)\n",
    "    if response:\n",
    "        print(\"Metrics the for first call to the LLM:\")\n",
    "        token_usage = response['usage']\n",
    "        input_tokens = token_usage['inputTokens']\n",
    "        print(f\"Input tokens:  {input_tokens}\")\n",
    "        output_tokens = token_usage['outputTokens']\n",
    "        print(f\"Output tokens:  {output_tokens}\")\n",
    "        total_tokens = token_usage['totalTokens']\n",
    "        print(f\"Total tokens:  {total_tokens}\")\n",
    "        latency = response['metrics']['latencyMs']\n",
    "        print(f\"Latency: {latency} ms\\n\")\n",
    "\n",
    "        # Check the LLM's response to see if it answered the question or needs to use the internet search tool\n",
    "        internet_search = None\n",
    "        for content in response['output']['message']['content']:\n",
    "            if isinstance(content, dict) and 'toolUse' in content:\n",
    "                tool_use = content['toolUse']\n",
    "                if tool_use['name'] == \"internet_search\":\n",
    "                    internet_search = tool_use['input']\n",
    "                    break\n",
    "\n",
    "        if internet_search:\n",
    "            question = internet_search[\"question\"]\n",
    "            # Call the function to get the content from the internet search provider\n",
    "            content = handle_search(question, search_provider)\n",
    "            if content:\n",
    "                print(\"\\nInternet search successful\")\n",
    "                response = answer_question_with_content(question, content)\n",
    "                if response:\n",
    "                    print(f\"\\nFinal answer = {response['output']['message']['content'][-1]['text']}\\n\")\n",
    "                else:\n",
    "                    print(\"Unable to get a response from Bedrock\")\n",
    "            else:\n",
    "                print(\"No content found from internet search\")\n",
    "        else:\n",
    "            print(\"No internet search needed.\")\n",
    "            answer = response['output']['message']['content'][-1]['text']\n",
    "            answer = answer.split(\"</thinking>\")[-1]\n",
    "            print(f\"Final answer is: {answer}\\n\")\n",
    "    else:\n",
    "        print(\"Unable to get a response from Bedrock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def answer_question_with_content(question, content):\n",
    "    query = f\"\"\"\n",
    "    Based solely on this content:\n",
    "    <content>\n",
    "    {content}\n",
    "    </content>\n",
    "    Answer this question:\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "    Skip any preamble or references to the tool.\n",
    "    \"\"\"\n",
    "\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": query}]}],\n",
    "        \"system\": [{ \"text\": \"You are an expert research assistant.\" }],\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": 4096,\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = call_bedrock(converse_api_params)\n",
    "    if response:\n",
    "        print(\"\\nMetrics the for call to the LLM including the internet search text:\")\n",
    "        token_usage = response['usage']\n",
    "        input_tokens = token_usage['inputTokens']\n",
    "        print(f\"Input tokens:  {input_tokens}\")\n",
    "        output_tokens = token_usage['outputTokens']\n",
    "        print(f\"Output tokens:  {output_tokens}\")\n",
    "        total_tokens = token_usage['totalTokens']\n",
    "        print(f\"Total tokens:  {total_tokens}\")\n",
    "        latency = response['metrics']['latencyMs']\n",
    "        print(f\"Latency: {latency} ms\")\n",
    "    \n",
    "        return response\n",
    "    else:\n",
    "        print(\"Unable to get a response from Bedrock\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Which country won the most gold medals in the 2020 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Which country won the most gold medals in the 2024 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is the current weather in Seattle, Wa right now?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is the current price on Amazon stock?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"How many Grizzly bears are living in Washington State?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Searching and scraping with Google search\n",
    "\n",
    "In this example we create an additional function to process the web pages based on URLs returned by the Google search:\n",
    "\n",
    "* get_google_page_content\n",
    "    * This function uses the BeautifulSoup module to parse the html content of a single website URL\n",
    "    * Then the text is processed to remove spaces, blank lines, and short lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the search provider variable\n",
    "search_provider = \"Google\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_google_page_content(url):\n",
    "    try:\n",
    "        # Supply different headers for your requests to avoid bot detection\n",
    "        user_agents = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 17_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/128.0.6613.98 Mobile/15E148 Safari/604.1\",\n",
    "        \"Mozilla/5.0 (iPad; CPU OS 17_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) CriOS/128.0.6613.98 Mobile/15E148 Safari/604.1\"\n",
    "        ]\n",
    "        user_agent = random.choice(user_agents)\n",
    "        \n",
    "        # Supply common html header elements for Chrome clients\n",
    "        headers = {\n",
    "            \"User-Agent\": user_agent,\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"Sec-Fetch-Dest\": \"document\",\n",
    "            \"Sec-Fetch-Mode\": \"navigate\",\n",
    "            \"Sec-Fetch-Site\": \"none\",\n",
    "            \"Sec-Fetch-User\": \"?1\",\n",
    "            \"Cache-Control\": \"max-age=0\",\n",
    "        }\n",
    "        # Check the URL to see if it is a link to a PDF doc and skip\n",
    "        # This code could be extended to also parse PDF docs rather than skipping\n",
    "        if \".pdf\" in url.split('/')[-1]:\n",
    "            print(f\"Found a PDF file: {url} skipping...\")\n",
    "            return \"skip page\"\n",
    "        else:\n",
    "            # Use the requests module to get the contents of the URL\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "    \n",
    "            if response:\n",
    "                # Parse HTML content\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                # Remove script and style elements\n",
    "                for script_or_style in soup([\"script\", \"style\"]):\n",
    "                    script_or_style.decompose()\n",
    "                # Get the text\n",
    "                text = soup.get_text()\n",
    "                # Break into lines and remove leading and trailing space on each\n",
    "                lines = (line.strip() for line in text.splitlines())\n",
    "                # Break multi-headlines into a line each\n",
    "                chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "                # Drop blank lines\n",
    "                no_blank_lines = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "                # Break into lines again and remove any short lines\n",
    "                lines = no_blank_lines.splitlines()\n",
    "                cleaned_text = \"\"\n",
    "                character_count = 0\n",
    "                for line in lines:\n",
    "                    if len(line) >= 20:\n",
    "                        cleaned_text += line\n",
    "                return cleaned_text\n",
    "            else:\n",
    "                raise Exception(\"No response from the web server.\")\n",
    "    except requests.exceptions.Timeout as timeout_err: \n",
    "        print(f\"Timeout on this URL: {url} skipping...\")\n",
    "        return \"skip page\"\n",
    "    except Exception as err:\n",
    "        print(f\"Error while requesting content from {url} skipping...: {err}\")\n",
    "        return \"skip page\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Which country won the most gold medals in the 2020 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Which country won the most gold medals in the 2024 olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is the current weather in Seattle, Wa right now?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"What is the current price on Amazon stock?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer_question(\"Who is favored to be the next Prime Minister of Canada?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
