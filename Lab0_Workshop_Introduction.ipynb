{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Responsible Real-time Web Search Application Workshop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "We often find customers interested in integrating web search capabilities into their AI assistant use cases, especially when they need the most update-to-date information that's available online, but was not available to the LLMs when they were trained.\n",
    "\n",
    "When creating applications like this without GenAI, you will have to define the logic of routing manually: when your application should search the web/when they should utilize the local knowledge etc. With [Bedrock Tool Use](https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html), we can use LLMs to intelligently determine for us. You can define and implement the tools, such as database, Lambda function, or some other software, for your applications, In your code, you call the tool on the model's behalf. In this scenario, we define the tool implementation to be APIs. You then continue the conversation with the model by supplying a message with the result from the tool. Finally, the model generates a response for the original message that includes the tool results that you sent to the model.\n",
    "\n",
    "[Amazon Bedrock Guardrails](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html) enables you to implement safeguards for your generative AI applications based on your use cases and responsible AI policies. You can create multiple guardrails tailored to different use cases and apply them across multiple foundation models (FM), providing a consistent user experience and standardizing safety and privacy controls across generative AI applications. For our web search use cases, we want to demonstrate how you can apply Guardrails on the search results \n",
    "\n",
    "In this workshop, we will demonstrate how to build a responsible real-time web search application utilizing Amazon Bedrock Tool Use feature, AWS Lambda functions and Amazon Bedrock Guardrails: \n",
    "\n",
    "- In Lab 0, we introduce the key concepts and services needed for this solution, and we provide an architectural diagram for you to understand how this application is built.\n",
    "- In Lab 1, we will show you how to build the real-time web search application using the concepts and services we introduced in Lab 0.\n",
    "- In Lab 2, we demonstrate how to integrate the Responsible AI to our application by adding Guardrails to the application we built in Lab 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conceptual overview\n",
    "\n",
    "### Function-Calling (Tool Use) with Converse API in Amazon Bedrock\n",
    "\n",
    "In this section, we'll explore how to perform Function-Calling through the use of Tools with the Converse API for Amazon Bedrock.\n",
    "\n",
    "The Converse or ConverseStream API is a unified structured text action for simplifying the invocations to Bedrock LLMs. It includes the possibility to define tools for implementing external functions that can be called or triggered from the LLMs.\n",
    "\n",
    "Let's start by installing a few requirements for running this example. We only need to run this cell the first time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.32.84 requires botocore==1.34.84, but you have botocore 1.35.14 which is incompatible.\n",
      "sagemaker 2.215.0 requires protobuf<5.0,>=3.12, but you have protobuf 5.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install -qU boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now import the relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running boto3 version: 1.35.14\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json, sys\n",
    "from datetime import datetime\n",
    "\n",
    "print('Running boto3 version:', boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a few variables. Make sure to adjust these parameters according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Using region:  us-east-1\n"
     ]
    }
   ],
   "source": [
    "modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "#modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "#modelId = 'cohere.command-r-plus-v1:0'\n",
    "#modelId = 'cohere.command-r-v1:0'\n",
    "#modelId = 'mistral.mistral-large-2402-v1:0'\n",
    "print(f'Using modelId: {modelId}')\n",
    "\n",
    "region = 'us-east-1'\n",
    "print('Using region: ', region)\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = region,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to define our tools through Python functions.\n",
    "\n",
    "In our example, we will define a tool for simulating a weather forecast lookup tool (get_weather). Note in our example we're just returning a constant weather forecast to illustrate the concept, but you could make it fully functional by connecting any weather service API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define your tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolsList:\n",
    "    #Define our get_weather tool function...\n",
    "    def get_weather(self, city, state):\n",
    "        #print(city, state)\n",
    "        result = f'Weather in {city}, {state} is 70F and clear skies.'\n",
    "        print(f'Tool result: {result}')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's structure our tools configuration for passing this information to our Converse API later. We have to clearly define the schema that our tools are expecting in the corresponding functions.\n",
    "\n",
    "Note that we can also define specific configurations such as the tool choice, which allow us to either let the LLM choose automatically (auto), or overriding a fixed tool to be called always. You can check more information on this parameter in the Bedrock Converse API documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the configuration for our tool...\n",
    "toolConfig = {'tools': [],\n",
    "'toolChoice': {\n",
    "    'auto': {},\n",
    "    #'any': {},\n",
    "    #'tool': {\n",
    "    #    'name': 'get_weather'\n",
    "    #}\n",
    "    }\n",
    "}\n",
    "\n",
    "toolConfig['tools'].append({\n",
    "        'toolSpec': {\n",
    "            'name': 'get_weather',\n",
    "            'description': 'Get weather of a location.',\n",
    "            'inputSchema': {\n",
    "                'json': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'city': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'City of the location'\n",
    "                        },\n",
    "                        'state': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'State of the location'\n",
    "                        }\n",
    "                    },\n",
    "                    'required': ['city', 'state']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a handy function for calling Bedrock with the Converse API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrating tools with Converse API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for caling the Bedrock Converse API...\n",
    "def converse_with_tools(messages, system='', toolConfig=toolConfig):\n",
    "    response = bedrock.converse(\n",
    "        modelId=modelId,\n",
    "        system=system,\n",
    "        messages=messages,\n",
    "        toolConfig=toolConfig\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready for setting up our orchestration flow. In this case, we'll make a first call to the LLM with the initial prompt from the user, and depending on the answer from the LLM we'll either call a tool (function calling) or end the interaction.\n",
    "\n",
    "Note that in the case the LLM indicates that it wants to run a tool (function calling), it will give us the information required of tool name and arguments for us to run the relevant tool in our code; i.e. The LLMs cannot run the tools automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for orchestrating the conversation flow...\n",
    "def converse(prompt, system=''):\n",
    "    #Add the initial prompt:\n",
    "    messages = []\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Initial prompt:\\n{json.dumps(messages, indent=2)}\")\n",
    "\n",
    "    #Invoke the model the first time:\n",
    "    output = converse_with_tools(messages, system)\n",
    "    print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Output so far:\\n{json.dumps(output['output'], indent=2, ensure_ascii=False)}\")\n",
    "\n",
    "    #Add the intermediate output to the prompt:\n",
    "    messages.append(output['output']['message'])\n",
    "\n",
    "    function_calling = next((c['toolUse'] for c in output['output']['message']['content'] if 'toolUse' in c), None)\n",
    "\n",
    "    #Check if function calling is triggered:\n",
    "    if function_calling:\n",
    "        #Get the tool name and arguments:\n",
    "        tool_name = function_calling['name']\n",
    "        tool_args = function_calling['input'] or {}\n",
    "        \n",
    "        #Run the tool:\n",
    "        print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Running ({tool_name}) tool...\")\n",
    "        tool_response = getattr(ToolsList(), tool_name)(**tool_args) or \"\"\n",
    "        if tool_response:\n",
    "            tool_status = 'success'\n",
    "        else:\n",
    "            tool_status = 'error'\n",
    "\n",
    "        #Add the tool result to the prompt:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        'toolResult': {\n",
    "                            'toolUseId':function_calling['toolUseId'],\n",
    "                            'content': [\n",
    "                                {\n",
    "                                    \"text\": tool_response\n",
    "                                }\n",
    "                            ],\n",
    "                            'status': tool_status\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        #print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Messages so far:\\n{json.dumps(messages, indent=2)}\")\n",
    "\n",
    "        #Invoke the model one more time:\n",
    "        output = converse_with_tools(messages, system)\n",
    "        print(f\"\\n{datetime.now().strftime('%H:%M:%S')} - Final output:\\n{json.dumps(output['output'], indent=2, ensure_ascii=False)}\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have everything setup and are ready for testing our function-calling bot.\n",
    "\n",
    "Let's try with a few sample prompts, each one with different needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Tool Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "23:35:41 - Initial prompt:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What is the weather like in Queens, NY?\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "\n",
      "23:35:43 - Output so far:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"toolUse\": {\n",
      "          \"toolUseId\": \"tooluse_zuYEBloJRYWXmcVy921lFw\",\n",
      "          \"name\": \"get_weather\",\n",
      "          \"input\": {\n",
      "            \"city\": \"Queens\",\n",
      "            \"state\": \"NY\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "23:35:43 - Running (get_weather) tool...\n",
      "Tool result: Weather in Queens, NY is 70F and clear skies.\n",
      "\n",
      "23:35:45 - Final output:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"The weather in Queens, NY is currently pleasant with clear skies and a temperature around 70F.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "23:35:45 - Initial prompt:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"What is the capital of France?\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "\n",
      "23:35:45 - Output so far:\n",
      "{\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": [\n",
      "      {\n",
      "        \"text\": \"The capital of France is Paris.\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"What is the weather like in Queens, NY?\",\n",
    "    \"What is the capital of France?\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    converse(\n",
    "        system = [{\"text\": \"You're provided with a tool that can get the weather information for a specific locations 'get_weather'; \\\n",
    "            only use the tool if required. Don't make reference to the tools in your final answer.\"}],\n",
    "        prompt = prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the LLM decides whether or not to call the get_weather tool depending on the question.\n",
    "\n",
    "You can further improve this example by playing with the system prompts. Also, you might want to switch the LLM and explore the differences.\n",
    "\n",
    "Note that the Bedrock Converse API allow us to seamlessly use the same code and structure for working with multiple model providers. No changes are needed in the syntax of our interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Bedrock Guardrails - Examples using the Python SDK\n",
    "\n",
    "Guardrails for Amazon Bedrock have multiple components which include Content Filters, Denied Topics, Word and Phrase Filters, and Sensitive Word (PII & Regex) Filters. For a full list check out the [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-create.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Guardrails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardrails for Amazon Bedrock have multiple components which include Content Filters, Denied Topics, Word and Phrase Filters, and Sensitive Word (PII & Regex) Filters. For a full list check out the [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-create.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('bedrock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '304b667c-4f23-4a6f-a1d7-d62e1a209864', 'HTTPStatusCode': 202, 'HTTPHeaders': {'date': 'Fri, 06 Sep 2024 23:36:00 GMT', 'content-type': 'application/json', 'content-length': '172', 'connection': 'keep-alive', 'x-amzn-requestid': '304b667c-4f23-4a6f-a1d7-d62e1a209864'}, 'RetryAttempts': 0}, 'guardrailId': '12mi2ngynyh9', 'guardrailArn': 'arn:aws:bedrock:us-east-1:054464910056:guardrail/12mi2ngynyh9', 'version': 'DRAFT', 'createdAt': datetime.datetime(2024, 9, 6, 23, 36, 0, 261990, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "create_response = client.create_guardrail(\n",
    "    name='fiduciary-advice',\n",
    "    description='Prevents the our model from providing fiduciary advice.',\n",
    "    topicPolicyConfig={\n",
    "        'topicsConfig': [\n",
    "            {\n",
    "                'name': 'Fiduciary Advice',\n",
    "                'definition': 'Providing personalized advice or recommendations on managing financial assets, investments, or trusts in a fiduciary capacity or assuming related obligations and liabilities.',\n",
    "                'examples': [\n",
    "                    'What stocks should I invest in for my retirement?',\n",
    "                    'Is it a good idea to put my money in a mutual fund?',\n",
    "                    'How should I allocate my 401(k) investments?',\n",
    "                    'What type of trust fund should I set up for my children?',\n",
    "                    'Should I hire a financial advisor to manage my investments?'\n",
    "                ],\n",
    "                'type': 'DENY'\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    contentPolicyConfig={\n",
    "        'filtersConfig': [\n",
    "            {\n",
    "                'type': 'SEXUAL',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'HIGH'\n",
    "            },\n",
    "            {\n",
    "                'type': 'VIOLENCE',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'HIGH'\n",
    "            },\n",
    "            {\n",
    "                'type': 'HATE',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'HIGH'\n",
    "            },\n",
    "            {\n",
    "                'type': 'INSULTS',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'HIGH'\n",
    "            },\n",
    "            {\n",
    "                'type': 'MISCONDUCT',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'HIGH'\n",
    "            },\n",
    "            {\n",
    "                'type': 'PROMPT_ATTACK',\n",
    "                'inputStrength': 'HIGH',\n",
    "                'outputStrength': 'NONE'\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    wordPolicyConfig={\n",
    "        'wordsConfig': [\n",
    "            {'text': 'fiduciary advice'},\n",
    "            {'text': 'investment recommendations'},\n",
    "            {'text': 'stock picks'},\n",
    "            {'text': 'financial planning guidance'},\n",
    "            {'text': 'portfolio allocation advice'},\n",
    "            {'text': 'retirement fund suggestions'},\n",
    "            {'text': 'wealth management tips'},\n",
    "            {'text': 'trust fund setup'},\n",
    "            {'text': 'investment strategy'},\n",
    "            {'text': 'financial advisor recommendations'}\n",
    "        ],\n",
    "        'managedWordListsConfig': [\n",
    "            {'type': 'PROFANITY'}\n",
    "        ]\n",
    "    },\n",
    "    sensitiveInformationPolicyConfig={\n",
    "        'piiEntitiesConfig': [\n",
    "            {'type': 'EMAIL', 'action': 'ANONYMIZE'},\n",
    "            {'type': 'PHONE', 'action': 'ANONYMIZE'},\n",
    "            {'type': 'NAME', 'action': 'ANONYMIZE'},\n",
    "            {'type': 'US_SOCIAL_SECURITY_NUMBER', 'action': 'BLOCK'},\n",
    "            {'type': 'US_BANK_ACCOUNT_NUMBER', 'action': 'BLOCK'},\n",
    "            {'type': 'CREDIT_DEBIT_CARD_NUMBER', 'action': 'BLOCK'}\n",
    "        ],\n",
    "        'regexesConfig': [\n",
    "            {\n",
    "                'name': 'Account Number',\n",
    "                'description': 'Matches account numbers in the format XXXXXX1234',\n",
    "                'pattern': r'\\b\\d{6}\\d{4}\\b',\n",
    "                'action': 'ANONYMIZE'\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    contextualGroundingPolicyConfig={\n",
    "        'filtersConfig': [\n",
    "            {\n",
    "                'type': 'GROUNDING',\n",
    "                'threshold': 0.75\n",
    "            },\n",
    "            {\n",
    "                'type': 'RELEVANCE',\n",
    "                'threshold': 0.75\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    blockedInputMessaging=\"\"\"I can provide general info about Acme Financial's products and services, but can't fully address your request here. For personalized help or detailed questions, please contact our customer service team directly. For security reasons, avoid sharing sensitive information through this channel. If you have a general product question, feel free to ask without including personal details. \"\"\",\n",
    "    blockedOutputsMessaging=\"\"\"I can provide general info about Acme Financial's products and services, but can't fully address your request here. For personalized help or detailed questions, please contact our customer service team directly. For security reasons, avoid sharing sensitive information through this channel. If you have a general product question, feel free to ask without including personal details. \"\"\",\n",
    "    tags=[\n",
    "        {'key': 'purpose', 'value': 'fiduciary-advice-prevention'},\n",
    "        {'key': 'environment', 'value': 'production'}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(create_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting a Guardrail, creating a version and listing all the versions and Drafts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will provide all the data about the DRAFT version we have\n",
    "get_response = client.get_guardrail(\n",
    "    guardrailIdentifier=create_response['guardrailId'],\n",
    "    guardrailVersion='DRAFT'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create a version for our Guardrail \n",
    "version_response = client.create_guardrail_version(\n",
    "    guardrailIdentifier=create_response['guardrailId'],\n",
    "    description='Version of Guardrail'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '0afb7094-9ccc-4563-961f-e826db2ddd90', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 06 Sep 2024 23:36:01 GMT', 'content-type': 'application/json', 'content-length': '632', 'connection': 'keep-alive', 'x-amzn-requestid': '0afb7094-9ccc-4563-961f-e826db2ddd90'}, 'RetryAttempts': 0}, 'guardrails': [{'id': '12mi2ngynyh9', 'arn': 'arn:aws:bedrock:us-east-1:054464910056:guardrail/12mi2ngynyh9', 'status': 'VERSIONING', 'name': 'fiduciary-advice', 'description': 'Prevents the our model from providing fiduciary advice.', 'version': 'DRAFT', 'createdAt': datetime.datetime(2024, 9, 6, 23, 36, 0, 261990, tzinfo=tzlocal()), 'updatedAt': datetime.datetime(2024, 9, 6, 23, 36, 0, 347578, tzinfo=tzlocal())}, {'id': '12mi2ngynyh9', 'arn': 'arn:aws:bedrock:us-east-1:054464910056:guardrail/12mi2ngynyh9', 'status': 'CREATING', 'name': 'fiduciary-advice', 'description': 'Version of Guardrail', 'version': '1', 'createdAt': datetime.datetime(2024, 9, 6, 23, 36, 1, 523282, tzinfo=tzlocal()), 'updatedAt': datetime.datetime(2024, 9, 6, 23, 36, 1, 523282, tzinfo=tzlocal())}]}\n"
     ]
    }
   ],
   "source": [
    "# To list the DRAFT version of all your guardrails, don’t specify the guardrailIdentifier field. To list all versions of a guardrail, specify the ARN of the guardrail in the guardrailIdentifier field.\n",
    "list_guardrails_response = client.list_guardrails(\n",
    "    guardrailIdentifier=create_response['guardrailArn'],\n",
    "    maxResults=5)\n",
    "\n",
    "print(list_guardrails_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing our Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"type\": \"message\",\n",
      "  \"role\": \"assistant\",\n",
      "  \"content\": [\n",
      "    {\n",
      "      \"type\": \"text\",\n",
      "      \"text\": \"I can provide general info about Acme Financial's products and services, but can't fully address your request here. For personalized help or detailed questions, please contact our customer service team directly. For security reasons, avoid sharing sensitive information through this channel. If you have a general product question, feel free to ask without including personal details. \"\n",
      "    }\n",
      "  ],\n",
      "  \"amazon-bedrock-trace\": {\n",
      "    \"guardrail\": {\n",
      "      \"input\": {\n",
      "        \"12mi2ngynyh9\": {\n",
      "          \"topicPolicy\": {\n",
      "            \"topics\": [\n",
      "              {\n",
      "                \"name\": \"Fiduciary Advice\",\n",
      "                \"type\": \"DENY\",\n",
      "                \"action\": \"BLOCKED\"\n",
      "              }\n",
      "            ]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"amazon-bedrock-guardrailAction\": \"INTERVENED\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Build our request to Bedrock, we will test our second version\n",
    "\n",
    "payload = {\n",
    "    \"modelId\": \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    \"contentType\": \"application/json\",\n",
    "    \"accept\": \"application/json\",\n",
    "    \"body\": {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"How should I invest for my retirement? I want to be able to generate $5,000 a month\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert the payload to bytes\n",
    "body_bytes = json.dumps(payload['body']).encode('utf-8')\n",
    "\n",
    "# Invoke the model\n",
    "response = bedrock.invoke_model(\n",
    "    body = body_bytes,\n",
    "    contentType = payload['contentType'],\n",
    "    accept = payload['accept'],\n",
    "    modelId = payload['modelId'],\n",
    "    guardrailIdentifier = create_response['guardrailId'], \n",
    "    guardrailVersion =\"1\", \n",
    "    trace = \"ENABLED\"\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "response_body = response['body'].read().decode('utf-8')\n",
    "print(json.dumps(json.loads(response_body), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we understand the key concepts, let's review the architecture for a responsible real-time web search application. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
